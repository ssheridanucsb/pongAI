{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(6, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(2, activation=None)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input layer paddle.xcor, paddle.ycor, ball.xcor, ball.ycor, ball.dx, ball.dy\n",
    "#4 hidden layers \n",
    "#output layer, up, down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-3\n",
    "MAX_ITERS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(model, observation):\n",
    "  # add batch dimension to the observation\n",
    "    observation = np.expand_dims(observation, axis=0)\n",
    "    logits = model.predict(observation) \n",
    "   # print(logits)\n",
    "    \n",
    "  \n",
    "    # pass the log probabilities through a softmax to compute true probabilities\n",
    "    prob_weights = tf.nn.softmax(logits).numpy()\n",
    "    #print(prob_weights)\n",
    "    action = np.random.choice(2, size=1, p=prob_weights.flatten())[0] \n",
    "   # print(action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self): \n",
    "        self.clear()\n",
    "    def clear(self): \n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "  # Add observations, actions, rewards to memory\n",
    "    def add_to_memory(self, new_observation, new_action, new_reward): \n",
    "        self.observations.append(new_observation)\n",
    "        self.actions.append(new_action)\n",
    "        self.rewards.append(new_reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x = x - np.mean(x)\n",
    "    x = x / np.std(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99): \n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    R = 0\n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "      # NEW: Reset the sum if the reward is not 0 (the game has ended!)\n",
    "        if rewards[t] != 0:\n",
    "            R = 0\n",
    "      # update the total discounted reward as before\n",
    "        R = R * gamma + rewards[t]\n",
    "        discounted_rewards[t] = R\n",
    "      \n",
    "    return normalize(discounted_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, actions, rewards): \n",
    "    neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions)\n",
    "    loss = tf.reduce_mean( neg_logprob * rewards ) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward propagate through the agent network\n",
    "        logits = model(observations)\n",
    "\n",
    "        loss = compute_loss(logits, actions, discounted_rewards) \n",
    "        \n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables) \n",
    "    \n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddle_a_up():\n",
    "    if paddle_a.ycor() < 275:\n",
    "        y = paddle_a.ycor()\n",
    "        y += 40\n",
    "        paddle_a.sety(y)\n",
    "\n",
    "def paddle_a_down():\n",
    "    if paddle_a.ycor() > -275:\n",
    "        y = paddle_a.ycor()\n",
    "        y -= 40\n",
    "        paddle_a.sety(y)\n",
    "\n",
    "def paddle_b_up():\n",
    "    if paddle_b.ycor() < 275:\n",
    "        y = paddle_b.ycor()\n",
    "        y += 40\n",
    "        paddle_b.sety(y)\n",
    "\n",
    "def paddle_b_down():\n",
    "    if paddle_a.ycor() > -275:\n",
    "        y = paddle_b.ycor()\n",
    "        y -= 40\n",
    "        paddle_b.sety(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle\n",
    "import os\n",
    "def draw_board():\n",
    "    wn = turtle.Screen()\n",
    "    wn.title(\"Pong\")\n",
    "    wn.bgcolor(\"black\")\n",
    "    wn.setup(width=800, height=600)\n",
    "    wn.tracer(0)\n",
    "\n",
    "\n",
    "    # Paddle A\n",
    "    paddle_a = turtle.Turtle()\n",
    "    paddle_a.speed(0)\n",
    "    paddle_a.shape(\"square\")\n",
    "    paddle_a.color(\"white\")\n",
    "    paddle_a.shapesize(stretch_wid=5,stretch_len=1)\n",
    "    paddle_a.penup()\n",
    "    paddle_a.goto(-350, 0)\n",
    "\n",
    "    # Paddle B\n",
    "    paddle_b = turtle.Turtle()\n",
    "    paddle_b.speed(0)\n",
    "    paddle_b.shape(\"square\")\n",
    "    paddle_b.color(\"white\")\n",
    "    paddle_b.shapesize(stretch_wid=5,stretch_len=1)\n",
    "    paddle_b.penup()\n",
    "    paddle_b.goto(350, 0)\n",
    "\n",
    "    # Ball\n",
    "    ball = turtle.Turtle()\n",
    "    ball.speed(0)\n",
    "    ball.shape(\"square\")\n",
    "    ball.color(\"white\")\n",
    "    ball.penup()\n",
    "    ball.goto(0, 0)\n",
    "    ball.dx = 15\n",
    "    ball.dy = 15\n",
    "\n",
    "    \n",
    "    return wn, paddle_a, paddle_b, ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "memoryA = Memory()\n",
    "memoryB = Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(a, b):\n",
    "    return np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(d):\n",
    "    return 800/d - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–Ž         | 18/500 [00:27<12:13,  1.52s/it]"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ca197606abb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mredward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mball\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mball\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxcor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mball\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mball\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msety\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mball\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mycor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mball\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/turtle.py\u001b[0m in \u001b[0;36msetx\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1806\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;36m10.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m240.00\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m         \"\"\"\n\u001b[0;32m-> 1808\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_goto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVec2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_position\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msety\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/turtle.py\u001b[0m in \u001b[0;36m_goto\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m   3156\u001b[0m                       (self.currentLineItem,\n\u001b[1;32m   3157\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentLine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3158\u001b[0;31m                       \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pointlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentLineItem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3159\u001b[0m                       self.items[:])\n\u001b[1;32m   3160\u001b[0m                       )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/turtle.py\u001b[0m in \u001b[0;36m_pointlist\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    753\u001b[0m         (9.9999999999999982, 0.0)]\n\u001b[1;32m    754\u001b[0m         >>> \"\"\"\n\u001b[0;32m--> 755\u001b[0;31m         \u001b[0mcl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0mpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;32mreturn\u001b[0m  \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mcoords\u001b[0;34m(self, *args, **kw)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36mcoords\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2467\u001b[0m         return [self.tk.getdouble(x) for x in\n\u001b[1;32m   2468\u001b[0m                            self.tk.splitlist(\n\u001b[0;32m-> 2469\u001b[0;31m                    self.tk.call((self._w, 'coords') + args))]\n\u001b[0m\u001b[1;32m   2470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Args: (val, val, ..., cnf={})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m         \u001b[0;34m\"\"\"Internal function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "for i_episode in tqdm(range(MAX_ITERS)):\n",
    "    reward = 0\n",
    "    wn, paddle_a, paddle_b, ball = draw_board()\n",
    "    done = False \n",
    "    while True:\n",
    "        #update screen\n",
    "        wn.update()\n",
    "            \n",
    "            \n",
    "        if ball.dx > 0:\n",
    "            \n",
    "            observation = np.array([paddle_b.xcor(), paddle_b.ycor(), ball.xcor(), ball.ycor(), ball.dx, ball.dy])\n",
    "            action = choose_action(modelB, observation)\n",
    "            if action ==1:\n",
    "                paddle_b_up()\n",
    "            elif action == 0:\n",
    "                paddle_b_down()\n",
    "            \n",
    "            b = np.array([paddle_b.xcor(), paddle_b.ycor()])\n",
    "            z = np.array([ball.xcor(), ball.ycor()])\n",
    "            d = dist(b,z)\n",
    "            reward = reward_func(d)\n",
    "        \n",
    "        elif ball.dx < 0:\n",
    "            observation = np.array([paddle_a.xcor(), paddle_a.ycor(), ball.xcor(), ball.ycor(), ball.dx, ball.dy])\n",
    "            action = choose_action(modelA, observation)\n",
    "            if action == 1:\n",
    "                paddle_a_up()\n",
    "            elif action == 0:\n",
    "                paddle_a_down()\n",
    "            a = np.array([paddle_a.xcor(), paddle_a.ycor()])\n",
    "            z = np.array([ball.xcor(), ball.ycor()])\n",
    "            d = dist(a,z)\n",
    "            reward = reward_func(d)\n",
    "        \n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "       \n",
    "        redward = 0\n",
    "        \n",
    "        ball.setx(ball.xcor() + ball.dx)\n",
    "        ball.sety(ball.ycor() + ball.dy)\n",
    "               \n",
    "\n",
    "        # Top and bottom\n",
    "        if ball.ycor() > 290:\n",
    "            ball.sety(290)\n",
    "            ball.dy *= -1\n",
    "\n",
    "        elif ball.ycor() < -290:\n",
    "            ball.sety(-290)\n",
    "            ball.dy *= -1\n",
    "\n",
    "        # Left and right\n",
    "        if ball.xcor() > 350:\n",
    "            done = True\n",
    "\n",
    "        elif ball.xcor() < -350:\n",
    "            done = True\n",
    "            \n",
    "            # Paddle and ball collisions\n",
    "        if ball.xcor() < -340 and ball.ycor() < paddle_a.ycor() + 50 and ball.ycor() > paddle_a.ycor() - 50:\n",
    "            ball.dx *= -1\n",
    "            \n",
    "        elif ball.xcor() > 340 and ball.ycor() < paddle_b.ycor() + 50 and ball.ycor() > paddle_b.ycor() - 50:\n",
    "            ball.dx *= -1\n",
    "        \n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            if memoryA.observations:\n",
    "                train_step(modelA, optimizer, \n",
    "                     observations=np.vstack(memoryA.observations),\n",
    "                     actions=np.array(memoryA.actions),\n",
    "                     discounted_rewards = discount_rewards(memoryA.rewards))\n",
    "            if memoryB.observations:\n",
    "                train_step(modelB, optimizer, \n",
    "                     observations=np.vstack(memoryB.observations),\n",
    "                     actions=np.array(memoryB.actions),\n",
    "                     discounted_rewards = discount_rewards(memoryB.rewards))\n",
    "                \n",
    "            memoryA.clear()\n",
    "            memoryB.clear()\n",
    "            turtle.resetscreen()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
